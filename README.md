In this project, we implemented the [Barlow Twins](https://arxiv.org/abs/2103.03230) method for self-supervised learning using the CIFAR-10 dataset. Self-supervised models have gained significant attention in recent years as they leverage large amounts of unlabeled data, allowing training in a manner similar to supervised learning but without the need for manually annotated labels. Instead, these models create surrogate tasks or objectives that encourage learning meaningful representations from the data itself. For example, by formulating tasks such as predicting missing parts of an input or generating contextually coherent outputs, the model learns to extract and encode high-level features and relationships inherent in the data. This approach enables the capture of rich, useful representations that can later be transferred to downstream tasks with labeled data. We trained the model for about 80 epochs on the training dataset with a batch size of 256. After training the self-supervised model, we utilized its learned representations to train a new supervised model. To do so, we split the test set into a new training set and a test set (with an 80/20 ratio) and used ResNet50 as the base architecture for the implementation. Ultimately, we achieved an accuracy of 68% on the training set and 64% on the validation dataset, demonstrating the potential of self-supervised learning techniques to improve representation quality for subsequent supervised tasks.
